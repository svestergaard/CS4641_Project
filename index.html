<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="./pages/styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Overpass:wght@700&family=Raleway:wght@200;400;900&display=swap" rel="stylesheet">
</head>
<body>
    <section id="hero">
        <img class="qb1" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/mattryan.png" alt="Matt Ryan">
        <img class="qb2" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/tombrady.png" alt="Tom Brady">
        <div class="wrapper">
            <h1>Predicting Future Performance of NFL Quarterbacks</h1>
            <h3>By Ryan Helgason, Michael Shealy, Sam Vestergaard & Corey Xing</h3>
            <div class="btn"><a href="https://github.com/svestergaard/CS4641_Project">View on Github</a></div>
        </div>        
    </section>
    <section class="intro">
        <div class="wrapper">
            <img src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/infographic.png" alt="Infographic">
            <h2>Introduction</h2> 
            <p>The primary goal of our project is to predict the future winrate of NFL quarterbacks based on their performance numbers from previous seasons. We selected the QB position because it is widely considered the position which has the single largest impact on team performance, and is also the position for which data is most readily available. The aim of this project is to produce an improved model for quantifying a QB's future value, which would allow NFL teams to make better decisions in roster construction. Current methods for projecting QB performance still rely heavily on traditional methods, such as scouting, which is subjective and prone to bias, so we believe such a model would provide valuable information in this regard. The dataset we will use originates from Pro-Football-Reference.com, and consists of QB counting stats such as touchdowns, interceptions, yardage, and much more for the past ~20 seasons.
            </p>
        </div>        
    </section>
    <section class="methods">
        <div class="wrapper">
            <h2>Methods Overview</h2> 
            <p>For this project, a dimensionality reduction method must first be used to determine what features from the many available to us are most important in terms of predicting future wins.
            Principal Component Analysis (PCA) will be used to find the features that explain the most variance in the target variable so that a more efficient model can be made afterwards. In this way, 
            less necessary features such as uniform number can be eliminated to allow better focus on relevant features. Ridge Regression is our initial choice for the supervised model that will 
            predict the amount of wins a quarterback will get in a future season. A regression model will predict an actual integer value for our target variable unlike a classification algorithm, 
            which is what we want in terms of predicting future wins. We also do not want sharp fluctuations in our predicted output, and using Ridge Regression will help optimize the complexity of 
            the model to ensure a smoother fit. To compare this result to other methods, we also plan to use a neural network which should perform better at solving the interactions between input variables.
            </p>       

            <h3>Supervised Methods</h3> 
            <p>For our supervised learning methods, we are planning to use ridge regression and random forest regression. We initially decided that regression would
                be the best way to approach this problem, since we are trying to predict a numerical result. However, we have also considered the possibility
                of approaching this problem from a classification standpoint rather than regression, since the range of possible target values is quite small. 
                Our clustering provided us with some early optimism in this regard, and we believe logistic regression and neural networks to be potentially valuable for
                this purpose.
            </p>
        </div>
    </section>
    <section class="data">
        <div class="wrapper">
            <h2>Data</h2>
            <p>The dataset we used was acquired from Pro-Football-Reference.com. It contains complete statistics for all quarterbacks dating back
            to 1985. The value we are trying to predict is the number of wins a quarterback will achieve in a subsequent season, so each data point 
            was labelled with that quarterback's win total in the immediate next season played (a possible range of 0 to 16). In order to ensure that all data points are valid,
            we filtered out all instances in which a quarterback did not start at least one game and throw at least 20 passes in two consecutive seasons.
            This ensures that both the current season statistics and next season win total are meaningful values. After doing this, a number of features were
            discarded from the dataset. This includes meaningless values such as player name or team name, that obviously have no bearing on QB performance. Certain
            other features, such as those related to field goal kicking were also discarded, since the vast majority of NFL QBs will never kick a field goal,
            and this stat is therefore, irrelevant to QB performance. Lastly, certain derived stats, such as PFR's Approximate Value stat, because these
            statistics are calculated from other features already in the dataset, therefore not providing additional information. After these steps, we were
            left with 36 features.</p>
            <br>
            <p>Before we could apply any of our chosen methods, we first had to split our dataset into training and testing sets, which we did using a 70-30 training to testing
                split. We also standardized our dataset such that each feature would have a mean of 0 and standard deviation of 1. This is because some of the methods we are using
                are sensitive to distance/magnitude, so we need to make sure that features are all on the same scale. 
            </p>
            <br>
            <p>The features remaining break down into the following:</p>
            <br>
            <ul class="col-3">
                <li><span>Year</span></li>
                <li><span>Age</span></li>
                <li><span>Ht:</span> Height</li>
                <li><span>Wt:</span> Weight</li>
                <li><span>BMI:</span> Body mass index</li>
                <li><span>G:</span> Games played</li>
                <li><span>GS:</span> Games started</li>
                <li><span>Cmp:</span> Passes completed</li>
                <li><span>Att:</span> Passes attempted</li>
                <li><span>Cmp%:</span> Completion rate</li>
                <li><span>Yds:</span> Passing yards</li>
                <li><span>TD:</span> Passing touchdowns</li>
            </ul>
            <ul class="col-3">
                <li><span>Int:</span> Interceptions thrown</li>
                <li><span>Pick6:</span> Interceptions thrown returned for touchdown</li>
                <li><span>TD%:</span> Passing touchdown rate</li>
                <li><span>Int%:</span> Interception rate</li>
                <li><span>Rate:</span> Passer rating</li>
                <li><span>Sk:</span> Sacks taken</li>
                <li><span>Yds.1:</span> Yards lost due to sacks</li>
                <li><span>Sk%:</span> Sack rate</li>
                <li><span>Y/A:</span> Yards per pass attempt</li>
                <li><span>Y/G:</span> Yards per game</li>
                <li><span>W:</span> Wins</li>
                <li><span>L:</span> Losses</li>
            </ul>
            <ul class="col-3">
                <li><span>T: Ties</li>
                <li><span>4QC:</span> 4th quarter comebacks</li>
                <li><span>GWD:</span> Game winning drives</li>
                <li><span>Att.1:</span> Rushing attempts</li>
                <li><span>Yds.2:</span> Rushing yards</li>
                <li><span>Y/A.1:</span> Rushing yards per attempt</li>
                <li><span>TD.1:</span> Rushing touchdowns</li>
                <li><span>Y/G.1:</span> Rushing yards per game</li>
                <li><span>2PM:</span> 2 point conversions</li>
                <li><span>Pts:</span> Total points (does not include passing TDs)</li>
                <li><span>PB:</span> Pro Bowl selections</li>
                <li><span>AP1:</span> 1st team All Pro selections</li>
            </ul>
        </div>
    </section>
    <section class="discussion">
        <div class="wrapper">
            <h2>Discussion</h2> 
            <p>The best possible outcome for our model is that it predicts long-term performance of NFL quarterbacks with high accuracy. If the model is able to surpass traditional scouting and 
            analysis methods, it could provide a new means of determining which players are worthy of investment. Machine analysis of player performance as far back as college games could provide 
            a much simpler means of drafting. If the model is able to perform well, a logical next step would be to expand the model to include other positions. Obviously, the features would be much
            different for each position, but it is not outside the realm of possibility of training the model on other features for the purpose of analyzing the performance of all NFL players.
            </p>
        </div>        
    </section>
    <section class="results">
        <div class="wrapper">
            <h2>Unsupervised Results</h2> 
            <p>Our hope is that our model provides a fairly accurate means of predicting quarterback performance in the long term. Currently, most quarterback analysis done through traditional scouting 
            and statistical analysis. Ideally, our model could provide a new means of predicting the utility of quarterbacks in the NFL.
            </p>
            <h3>Linearly Independent Features</h3>
            <p>As expected after manually cleaning the data and removing calculated features, the number of completely linearly indepent features (correlation != 1.0) is equivalent to the total number of features in our dataset. That said, as shown in the correlatoin matrix below, there are several features that are closely related including some with a correlation of over 0.9. While we decided to keep all of these features included in the dataset, a recommendation for the future would be to set a lower criteria for classifying linear dependency.</p>
            <img class="center" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/corr_matrix.png" alt="Correlation Matrix">
            <h3>K-Means Analysis</h3>
            <p>
                We wanted to perform unsupervised clustering on our dataset to see if we could identify any trends in the data. With a perfect dataset, we would expect that K-Means would group data points into 17 distinct groups,
                corresponding to the number of projected wins 0 through 16. Obviously this is unrealistic, but it sets a useful baseline for how accurately our data points map to their respective labels. First, we analyzed the loss
                for each number of clusters in a specific range.
            </p>
            <img class="center" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/kmeans-elbow.png" alt="K-Means Elbow Method">
            <p>
                The ideal amount of clusters is not too obvious with this visualization, but the elbow method suggests that it lies around five clusters. Next, we output the results of K-means using specifically 5 clusters. The data below
                shows that the clusters are spread somewhat evenly, but the more concerning visualization to us was how well these clusters mapped to average wins. If each cluster had been created well, then we would expect to see a smooth
                gradient of wins between each cluster. With the exception of a couple of columns, this proved to be fairly true. These results for K-means with 5 clusters are shown below.
            </p>
            <img class="half" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/kmeans-visual-5.png" alt="K-Means Visualization">
            <img class="half" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/kmeans-avg-5.png" alt="Average Wins Per Cluster">
            <p>
                As mentioned earlier, a perfect dataset could map all players into the correct grouping of 17 clusters. Although the elbow method suggested 5 clusters, we wanted to view the dataset from this perspective too. The output shows
                is again mapped somewhat evenly. Analyzing how well clusters mapped to our ideal gradient of 0 through 16 wins, we found a reasonable spread of average wins. The average wins per cluster range from roughly 2 to 10 wins. While
                this is not perfect, it is well enough of a spread to prove features hold relevance when projecting season wins. These results for K-means with 17 clusters are shown below.
            </p>
            <img class="center_wide" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/kmeans-visual-17.png" alt="K-Means Visualization">
            <img class="center_wide" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/kmeans-avg-17.png" alt="Average Wins Per Cluster">
          <h3>Principal Component Analysis</h3>
            <p>To retain 99% of the variance of the original training data, we determined by analyzing its singular values that the required dimensionality reduces from 36 to 24. To recover 95% of the variance, it reduces to 18  â€” half of the original dimension. Moving forward, this will likely improve our regression models and other supervised learning correlation methods since there are less features to compare. The scree plot below shows the breakdown of the proportion of variance among the first 18 components. The adjacent table shows these values along with the most relevant feature within each principal component. 
            <img id="scree" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/scree.png" alt="Scree Plot">
            <img id="pca_table" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/table.png" alt="PCA Table">
            <p><br>The 3 bar charts below show a more detailed breakdown of the 36 original features within the first 3 principal components.<br><br></p>
            <img src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/bars.png" alt="PCA Bar Charts">
        </div>        
    </section>
    <section class="results2">
        <div class="wrapper">
            <h2>Supervised Results</h2> 
            <p>Our work with unsupervised machine learning helped us trim our features down to only the most important ones, as well as visualize how quarterbacks can be grouped based on
                specific features. We next moved towards supervised learning to design models that can predict quarterback wins in a single season. We tried a variety of supervised methods
                to this end, the results of which are shown below.
            </p>
            <img class="center_wide" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/SupervisedTable.PNG" alt="Supervised Learning Results">
            <h3>Logistic Regression</h3>
            <p>Logistic Regression was tested to build a simple linear classifier to predict the number of wins the following year. When used to predict the exact number of wins the following season,
          the model performed very poorly with an accuracy of 10.9% and an average F1 score of 0.069. The model was then built to predict certain win ranges for the following season. As shown above, 
          the model was tested on 4 equally sized win ranges (0-4 wins, 5-8, 9-12, and 13-16) and 2 equally sized win ranges (0-8 wins and 8-16). This significantly increased the accuracy and F1 score 
          of the model compared to the the no-grouping classification. The 2-class classification had the highest accuracy and F1-score out of the logistic regression models tested. The F1-score is lower 
          than the accuracy for each of the win ranges. When combined with the fact that the dataset does have class imbalance (there are significantly more samples with a small number of wins compared to a 
          large number of wins), this does not bode well for the model performance. One attribute about the logisitic regression model that makes it more useful than other models is that a probability distribution 
          is made for how likely a quarterback is going to have each of the specified win ranges. This is a useful output for owners and general managers to have when seeing how likely each of the outcomes the 
          following season could have.</p>
            <h3>Ridge Regression</h3>
            <p>Ridge regression was used to analyze how a different regression method could perform on our dataset in comparison to logistic regression. The main difference to note is that
                logistic regression is a classification model, whereas ridge regression predicts the number of wins as a continuous value. Obviously, this isn't super indicative of anything
                for our dataset, so we round these values to make them more useful. When predicting exactly how many wins a quarterback would get in the following season, this model achieved
                an accuracy of 11.5% and an F-score of 0.08. When using 4-class classification, this model achieved an accuracy of 43.8% and an F-score of 0.31. When using 2-class
                classification, this model achieved an accuracy of 74.6% and an F-score of 0.58. These results are essentially on par with our other models. If anything, it performed slightly
                more poorly than the other three supervised models we used. Ridge regression also showed an average r-squared value of 0.27.
            </p>
            <h3>Random Forest</h3>
            <p>We also attempted the Random Forest method to classify the future wins which, due to its randomness and probabalistic approach, should lessen the impact of overfitting issues with the training data. After experimenting over a range of hyperparameters, the best outcomes for the three data breakdowns are shown in the table. The accuracies and F-scores closely resemble those obtained with the other methods with around a 76% accuracy for the two-class set and around 12% for the full range of wins. One interesting advantage to this method is the ability to analyze the most relevant and important features used in classifying the data. Shown below, the top three features were the yards advanced per game by the quarterback, the number of team wins in the current season where the quarterback is a starting player, and the total yards advanced in the current season by the quarterback.</p>
            <img class="center" src="https://raw.githubusercontent.com/svestergaard/CS4641_Project/master/pages/images/important_features.PNG" alt="Most Important Features">
            <h3>Neural Network</h3>
            <p>Text</p>
        </div>        
    </section>
    <section class="analysis">
        <div class="wrapper">
            <h2>Analysis</h2>
                <h3>Challenges Faced</h3>
                <p>
                    After utilizing a variety of supervised techniques on our dataset, we found similar accuracies for all of them. This suggested that we were essentially maxing out on how
                    powerful our model could be given our dataset and revised features. We wanted to see if any specific changes to the models could improve our accuracy, if even by just a little
                    bit. We analyzed which datapoints were most frequently being misclassified by our supervised models to see if there were any outstanding issues. One thing that stood out to us
                    was that our model struggled with seasons where a quarterback was injured or played fewer games for any other reason. Because our models predict the wins for the following
                    season, the models would almost certainly underestimate how many wins the quarterback would achieve in the next season.
                </p>
                <p>
                    We also recognize that our models are limited by the size of our dataset. There is not really a good solution to this issue because there is only so much NFL data available.
                    Our source does a good job of compiling all NFL data that exists, so the only upside here is that our model could theoretically improve as time goes on. The last challenge for
                    consideration is that our models are unable to consider the impact of coaches or other players on the team. While it is commonly agreed that the quarterback is the most important
                    position, other individuals certainly have an affect as well.
                </p>
                <h3>Potential Improvements</h3>
                <p>
                    The first improvement that we would like to make in the future is to use a more conservative limit for linear dependency. When we first started unsupervised learning,
                    we eliminated all features that were found to be linearly dependent with some other feature. While this helped focus in on the most important features, it does not
                    consider features that, although linearly dependent, are very similar. A good example of this is touchdowns and points scored. With very little variation, the total points
                    scored by a quarterback can be expected to be 6 times as many touchdowns that were scored. However, the linear dependency sat around 0.995, probably due to 2-point conversions
                    or something similar.
                </p>
        </div>
    </section>
    <section class="references">
        <div class="wrapper">
            <h2>References</h2>  
            <ul>
            <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978585/">A Multilevel Multiset Time-Series Model for Describing Complex Developmental Processes</a></li>
            <li><a href="http://pzs.dstu.dp.ua/DataMining/pca/bibl/Principal%20components%20analysis.pdf">Principal Component Analysis</a></li>
            <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4529984/">The Current and Future Use of Ridge Regression for Prediction in Quantitative Genetics</a></li>
            </ul>
        </div>        
    </section>
    <script src="./pages/main.js"></script>
</body>
</html>


